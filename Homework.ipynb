{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bray1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bray1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bray1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bray1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_url = 'https://public-api.wordpress.com/rest/v1.1/sites/binaykumarray.wordpress.com/posts/'\n",
    "\n",
    "def scrape_blog(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    blog_data = json.loads(response.read())\n",
    "    data = [{i['slug']:{'title': i['title'], 'url': i['URL'], 'content': i['content']}} for i in blog_data.get('posts')]\n",
    "    content_list = [clean_html(i['content']) for i in blog_data.get('posts')]\n",
    "    return data, content_list\n",
    "\n",
    "def clean_html(html_text):\n",
    "    text = BeautifulSoup(html_text, \"lxml\").text\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "data, content_list = scrape_blog(blog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 22.237\n",
      "Iteration  1, inertia 11.911\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 1.938224e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")\n",
    "x = vectorizer.fit_transform(content_list)\n",
    "km = KMeans(n_clusters=5, init=\"k-means++\", max_iter=100, n_init=1, verbose=True)\n",
    "km.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([4, 7, 3, 3, 3]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) + ['million', 'billion', 'year', 'millions', 'billions', 'y/y', \"'s\", \"'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    oneDocument = content_list[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(5):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent = [word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = {}\n",
    "for cluster in range(5):\n",
    "    other_clusters = list(set(range(3)) - set([cluster]))\n",
    "    \n",
    "    keys_other_cluster = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster]) - keys_other_cluster\n",
    "    unique_keys[cluster] = nlargest(10, unique, key=counts[cluster].get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['coding',\n",
       "  '3.x',\n",
       "  'version',\n",
       "  'python3',\n",
       "  'even',\n",
       "  'better',\n",
       "  'properly',\n",
       "  'want',\n",
       "  '3.4',\n",
       "  'standards'],\n",
       " 1: [\"''\",\n",
       "  'response',\n",
       "  'thread',\n",
       "  'threads',\n",
       "  'user',\n",
       "  'http',\n",
       "  '8217',\n",
       "  '--',\n",
       "  'processes',\n",
       "  '``'],\n",
       " 2: ['list', '2', '4', '1', 'x', '8', 'append', '3', 'b', 'element'],\n",
       " 3: ['name',\n",
       "  'gc',\n",
       "  'bar',\n",
       "  'p2',\n",
       "  '__',\n",
       "  'variable',\n",
       "  'style',\n",
       "  'c1',\n",
       "  'parent',\n",
       "  'classes'],\n",
       " 4: ['changed',\n",
       "  '”',\n",
       "  '“',\n",
       "  'libraries',\n",
       "  'method',\n",
       "  'os.path.dirname',\n",
       "  'settings',\n",
       "  'work',\n",
       "  '__file__',\n",
       "  'step']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 22.463\n",
      "Iteration  1, inertia 11.923\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 1.938224e-07\n",
      "Clusters:  (array([0, 1, 2, 3, 4]), array([4, 7, 3, 3, 3]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['coding',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'properly',\n",
       " 'standards',\n",
       " 'flake8',\n",
       " 'functions',\n",
       " 'want',\n",
       " 'suggest',\n",
       " 'doc-strings']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MachineModel(object):\n",
    "    def __init__(self, cluster=3):\n",
    "        self.cluster = cluster\n",
    "        self.vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")\n",
    "        self.km = KMeans(n_clusters=5, init=\"k-means++\", max_iter=100, n_init=1, verbose=True)\n",
    "        self.classifier = KNeighborsClassifier(n_neighbors=cluster)\n",
    "        self._stopwords = set(\n",
    "            stopwords.words('english') +\n",
    "            list(punctuation) +\n",
    "            ['--', \"''\", 'p2', '4', 'x', '8', '10', '3', 'b', '8217', '”','“', \"'\", '–','3.x', '__', '5', '6', '``', '1']\n",
    "        )\n",
    "        \n",
    "    def add_vector_data(self, data):\n",
    "        self.data = data\n",
    "        self.vectordata = self.vectorizer.fit_transform(data)\n",
    "    \n",
    "    def fit_km(self):\n",
    "        self.km.fit(self.vectordata)\n",
    "    \n",
    "    def get_unique_cluster_data(self):\n",
    "        return np.unique(km.labels_, return_counts=True)\n",
    "        \n",
    "    def set_cluster_for_each_data(self):\n",
    "        self.text = {}\n",
    "        for i, cluster in enumerate(self.km.labels_):\n",
    "            oneDocument = self.data[i]\n",
    "            if cluster not in self.text.keys():\n",
    "                self.text[cluster] = oneDocument\n",
    "            else:\n",
    "                self.text[cluster] += oneDocument\n",
    "                \n",
    "    def _calculate_count_for_cluster(self):\n",
    "        self.keywords = {}\n",
    "        self.counts = {}\n",
    "        for cluster in range(self.cluster):\n",
    "            word_sent = word_tokenize(self.text[cluster].lower())\n",
    "            word_sent = [word for word in word_sent if word not in self._stopwords]\n",
    "            freq = FreqDist(word_sent)\n",
    "            self.keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "            self.counts[cluster] = freq\n",
    "        \n",
    "        self.unique_keys = {}\n",
    "        for cluster in range(self.cluster):\n",
    "            other_clusters = list(set(range(self.cluster)) - set([cluster]))\n",
    "\n",
    "            keys_other_cluster = set(self.keywords[other_clusters[0]]).union(set(self.keywords[other_clusters[1]]))\n",
    "            unique = set(self.keywords[cluster]) - keys_other_cluster\n",
    "            self.unique_keys[cluster] = nlargest(10, unique, key=self.counts[cluster].get)\n",
    "    \n",
    "    def _fit_data_to_classifier(self):\n",
    "        self.classifier.fit(self.vectordata, self.km.labels_)\n",
    "    \n",
    "    def train_data(self, data):\n",
    "        self.add_vector_data(data)\n",
    "        self.fit_km()\n",
    "        print(\"Clusters: \", self.get_unique_cluster_data())\n",
    "        self.set_cluster_for_each_data()\n",
    "        self._calculate_count_for_cluster()\n",
    "        self._fit_data_to_classifier()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        test = self.vectorizer.transform([data])\n",
    "        return self.classifier.predict(test)\n",
    "    \n",
    "\n",
    "article= \"\"\"List Comprehension is a beautiful feature of Python. But most of the Python beginners (like me) get confused when it comes to nested list comprehension. So I thought to write few lines of Python code that might help you to understand it better. Can you figure out the output of the following statement? [(x, y) for x in range(1, 5) for y in range(0, x)] Here is the output: [(1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2), (4, 0), (4, 1), (4, 2), (4, 3)] If you could figure it out correctly, you don't need to read the rest of the post. This is actually same as [(x, y) for x in range(1, 5) for y in range(0, x)] Now think about it, experiment with your own ideas and things will be clear.\"\"\"\n",
    "obj = MachineModel(5)\n",
    "obj.train_data(content_list)\n",
    "x=obj.predict(article)\n",
    "obj.unique_keys[x[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
